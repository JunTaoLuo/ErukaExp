# -*- coding: utf-8 -*-
"""TrOCRTraining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZwQlUeODyAbpav-n3YR8T2gZ1qQ2J6uv

# Setup
"""

# Imports

from transformers import (
    TrOCRConfig,
    TrOCRProcessor,
    TrOCRForCausalLM,
    ViTConfig,
    ViTModel,
    VisionEncoderDecoderModel,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments
)

from PIL import Image

import csv
import fnmatch
import os
import torch
from tqdm import tqdm
from torch.utils.data import DataLoader, Dataset
from datasets import load_metric

# Processor
# TODO: should this be trained?
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten")

script_dir = os.path.dirname(__file__)

"""# Ownership Card Dataset"""

class OCDataset(torch.utils.data.Dataset):
    @staticmethod
    def parse_csv(filepath):
        labels = {}
        with open(filepath) as f:
            f_csv = csv.reader(f)
            next(f_csv, None)  # skip the headers
            for row in f_csv:
                labels[row[0]] = row[1]
        return labels

    def __init__(self, processor, max_target_length=16) -> None:
        test_dir = f"{script_dir}/data/"
        labels_dict = self.parse_csv(f"{test_dir}labels.csv")
        input_files = fnmatch.filter(os.listdir(f"{test_dir}images/"), "*.jpg")

        self.X = []

        for input_file in input_files:
            # Extract pixel values
            file_path = f"{test_dir}images/{input_file}"
            # pixel_values.to(device)

            # Resolve label values
            parcel = input_file[:-4]
            raw_label = labels_dict[parcel]
            labels = processor.tokenizer(raw_label,
                padding="max_length",
                max_length=max_target_length).input_ids
            # important: make sure that PAD tokens are ignored by the loss function
            labels = [label if label != processor.tokenizer.pad_token_id else -100 for label in labels]
            # labels.to(device)

            self.X.append({"file_path": file_path, "parcel": parcel, "raw_labels": raw_label, "labels": torch.tensor(labels)})

    def __len__(self):
        return len(self.X)

    def __getitem__(self, ind):
        x = self.X[ind]
        image = Image.open(x["file_path"]).convert("RGB")
        pixel_values = processor(image, return_tensors="pt").pixel_values.squeeze(0)

        return {"pixel_values": pixel_values, "parcel": x["parcel"], "raw_labels": x["raw_labels"], "labels": x["labels"]}

oc_dataset = OCDataset(processor)


"""# Fine Tuning"""

print(f"OC Dataset: {len(oc_dataset)}")
encoding = oc_dataset[0]
print(encoding)
image = encoding["pixel_values"]
print(f"pixel_values: {image.shape}")

model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-stage1")

# set special tokens used for creating the decoder_input_ids from the labels
model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
model.config.pad_token_id = processor.tokenizer.pad_token_id
# make sure vocab size is set correctly
model.config.vocab_size = model.config.decoder.vocab_size

# set beam search parameters
model.config.eos_token_id = processor.tokenizer.sep_token_id
model.config.max_length = 64
model.config.early_stopping = True
model.config.no_repeat_ngram_size = 3
model.config.length_penalty = 2.0
model.config.num_beams = 4
model.config.output_scores = True

training_args = Seq2SeqTrainingArguments(
    predict_with_generate=True,
    evaluation_strategy="steps",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    # fp16=True,
    output_dir="./",
    logging_steps=2,
    save_steps=1000,
    eval_steps=200,
)

cer_metric = load_metric("cer")

def compute_metrics(pred):
    labels_ids = pred.label_ids
    pred_ids = pred.predictions

    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)
    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id
    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)

    cer = cer_metric.compute(predictions=pred_str, references=label_str)

    return {"cer": cer}

from transformers import default_data_collator

train_size = 500
eval_size = 100
test_size = len(oc_dataset) - train_size - eval_size
train_dataset, eval_dataset, test_dataset = torch.utils.data.random_split(oc_dataset, [train_size, eval_size, test_size])

"""# Evaluation"""

def evaluate_dataset(model, dataset):
    correct = 0
    batch_bar = tqdm(total=len(dataset), dynamic_ncols=True, leave=False, position=0, desc='Eval')
    generate_args = {"max_length": 16, "num_beams": 4, "return_dict_in_generate": True, "output_scores": True}

    for data in dataset:
        pixel_values = data["pixel_values"].unsqueeze(0)
        outputs = model.generate(pixel_values, **generate_args)
        generated_ids = outputs.sequences
        scores = outputs.sequences_scores
        print(scores)
        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
        generated_text = generated_text.translate({ord(c): None for c in ' ,.'})
        print(generated_text)
        target_labels = data["raw_labels"]
        parcel = data["parcel"]

        if target_labels == generated_text:
            correct += 1
        else:
            print(f"parcel: {parcel} target: {target_labels} predicted: {generated_text}\n")

        batch_bar.update()

    batch_bar.close()
    print("Eval complete\n")

    return correct/len(dataset)

model.load_state_dict(torch.load(f"{script_dir}/data/oc-carb-fine-tuning-8k.pt", map_location=torch.device('cpu')))
model.eval()

acc = evaluate_dataset(model, test_dataset)
print(f"Accuracy: {acc}\n")
